import tensorflow as tf
import numpy as np
from models import QNetwork

config = tf.ConfigProto()
config.gpu_options.allow_growth = True
tf.enable_eager_execution(config=config)


def huber_loss(x, delta=1.0):
    return tf.where(
        tf.abs(x) < delta,
        tf.square(x) * 0.5,
        delta * (tf.abs(x) - 0.5 * delta)
    )


class DEEPQ(tf.keras.Model):
    def __init__(self, observation_shape, num_actions, beta, optimizer, batch_size=32, grad_norm_clipping=None,
                 gamma=1.0, double_q=False, param_noise=False, param_noise_filter_func=None):
        super(DEEPQ, self).__init__()
        self.observation_shape = observation_shape
        self.num_actions = num_actions
        self.gamma = gamma
        self.double_q = double_q
        self.param_noise = param_noise
        self.param_noise_filter_func = param_noise_filter_func
        self.grad_norm_clipping = grad_norm_clipping
        self.beta = beta
        self.batch_size = batch_size
        self.optimizer = optimizer

        with tf.name_scope('q_network'):                             # observation_shape = (None,84,84,4)
            self.q_network = QNetwork(num_actions=num_actions)
        with tf.name_scope('target_q_network'):
            self.target_q_network = QNetwork(num_actions=num_actions)
        self.eps = tf.Variable(0., name="eps")

        # new add
        self.batchnum = 0
        self.epi_len = 0
        self.batch_count = 0
        self.epi_state = None
        self.epi_actions = None
        self.epi_rewards = None
        self.epi_terminals = None
        self.Q_tilde = None
        self.y_ = None

    def step(self, obs, stochastic=True, update_eps=-1):
        q_values = self.q_network(obs)  # shape = (None, n_actions)
        deterministic_actions = tf.argmax(q_values, axis=1)  # 计算 Q-value 并选择Q值最大的动作
        batch_size = tf.shape(obs)[0]
        # return a tensor with shape (batch_size,), each element in [0, num_actions).
        random_actions = tf.random.uniform(tf.stack([batch_size]), minval=0, maxval=self.num_actions, dtype=tf.int64)
        # return a tensor with shape (batch_size,), each element in True or False. (epsilon-greedy)
        chose_random = tf.random.uniform(tf.stack([batch_size]), minval=0, maxval=1, dtype=tf.float32) < self.eps
        # choose action by random_actions or deterministic actoins
        stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)

        if stochastic:
            output_actions = stochastic_actions
        else:
            output_actions = deterministic_actions

        if update_eps >= 0:  # update_eps is generated by LinearSchedule
            self.eps.assign(tf.convert_to_tensor(update_eps))
        return output_actions[0].numpy()

    def train_step(self, obs0, actions, y):
        # Gradient descend step to update the parameters of the main network
        # print("train:", obs0.shape, actions.shape, y.shape)
        with tf.GradientTape() as tape:
            # compute Q(s, a)
            q_t = self.q_network(obs0)                     # (None, num_actions)
            q_t_selected = tf.reduce_sum(q_t * tf.one_hot(actions, self.num_actions, dtype=tf.float32), 1)  # (None, )
            # huber loss
            td_error = q_t_selected - tf.stop_gradient(y)     # y.shape=(None,), td_error.shape=(None,)
            loss = tf.reduce_mean(huber_loss(td_error))       # huber_loss(td_error).shape=(None,), loss=()

        grads = tape.gradient(loss, self.q_network.trainable_variables)
        grads_clip, grads_norm = tf.clip_by_global_norm(grads, self.grad_norm_clipping)
        self.optimizer.apply_gradients(zip(grads_clip, self.q_network.trainable_variables))
        return loss

    def train_ebu(self, replay_buffer):
        # if all minibatches of previous episode is updated,
        # sample a new episode to create a new temporary target Q-table, Q_tilde
        # print("\n ----")
        # print("self.batchnum:", self.batchnum, ", self.batch_count:", self.batch_count, ", stored:",
        #       self.epi_state.shape if self.epi_state is not None else 0)
        if self.batchnum == self.batch_count:
            self.Q_tilde = np.array([])  # temporary target Q-table of next states S'
            # print("sample:")
            self.epi_state, self.epi_actions, self.epi_rewards, self.batchnum, self.epi_terminals = replay_buffer.sample()  # sample a new episode
            self.epi_len = self.batchnum * self.batch_size
            self.epi_state = tf.constant(self.epi_state)
            # print("sample s:", self.epi_state.shape, ", a:", len(self.epi_actions), ", r:", len(self.epi_rewards),
            #       ", d:", len(self.epi_terminals), ", batchnum=", self.batchnum)
            # print("First and second state:", np.array_equal(self.epi_state[0], self.epi_state[1]))
            for i in range(self.batchnum):
                self.Q_tilde = np.append(self.Q_tilde, self.target_q_network(
                    self.epi_state[self.batch_size * i:self.batch_size * (i + 1)]))
                # print(len(self.Q_tilde), self.Q_tilde[0].shape)
            self.Q_tilde = np.reshape(self.Q_tilde, (self.batchnum * self.batch_size, self.num_actions))
            self.Q_tilde = np.roll(self.Q_tilde, self.num_actions)  # the first row become the second, and the last row becomes the first.
            for i in range(self.epi_len):
                if self.epi_terminals[i]:
                    self.Q_tilde[i, :] = [0] * self.num_actions

            self.y_ = np.zeros(self.epi_len, dtype=np.float32)  # target value

            for i in range(0, self.epi_len):
                if i < self.epi_len - 1:
                    #  The last minibatch stores some redundant transitions of the second episode to fill a minibatch,
                    #  so a terminal most likely occurs before self.epi_len
                    if self.epi_terminals[i]:
                        self.y_[i] = self.epi_rewards[i]
                        self.Q_tilde[i + 1, self.epi_actions[i]] = self.beta * self.y_[i] + (1 - self.beta) * \
                                                                   self.Q_tilde[i + 1, self.epi_actions[i]]
                    elif self.epi_terminals[i + 1]:
                        self.y_[i] = self.epi_rewards[i] + self.gamma * np.max(self.Q_tilde[i])
                        self.Q_tilde[i + 1, :] = 0
                    else:
                        self.y_[i] = self.epi_rewards[i] + self.gamma * np.max(self.Q_tilde[i])
                        self.Q_tilde[i + 1, self.epi_actions[i]] = self.beta * self.y_[i] + (1 - self.beta) * \
                                                                   self.Q_tilde[i + 1, self.epi_actions[i]]
                if i == self.epi_len - 1:  # Most likely to be a transition of a redundant episode
                    if self.epi_terminals[i]:
                        self.y_[i] = self.epi_rewards[i]
                    else:
                        self.y_[i] = self.epi_rewards[i] + self.gamma * np.max(self.Q_tilde[i])

            self.batch_count = 1
            loss = self.train_step(
                self.epi_state[0:self.batch_size],                                                 # state
                tf.constant(np.reshape(self.epi_actions[0:self.batch_size], (self.batch_size,))),  # action
                tf.constant(np.reshape(self.y_[0:self.batch_size], (self.batch_size,))))           # target

        # if an episode is still being updated, use the next minibatch of the already generated target value.
        else:
            self.batch_count += 1
            loss = self.train_step(
                self.epi_state[(self.batch_count - 1) * self.batch_size:self.batch_count * self.batch_size],
                tf.constant(np.reshape(self.epi_actions[(self.batch_count - 1) * self.batch_size:self.batch_count * self.batch_size], (self.batch_size,))),
                tf.constant(np.reshape(self.y_[(self.batch_count - 1) * self.batch_size:self.batch_count * self.batch_size], (self.batch_size,))))
        return loss

    def update_target(self):
        self.target_q_network.set_weights(self.q_network.get_weights())
